# Barnes-Hut N-Body Simulation with MPI

This project implements the **Barnesâ€“Hut algorithm** for the Nâ€‘body problem using **MPI** for distributed parallelism. The focus is learning **MPI**, **distributed computing methods**, and **programming for performance/speedup**.

---

## ğŸ“– Overview
The Nâ€‘body problem models gravitational interactions among particles (e.g., stars/planets).
- **NaÃ¯ve**: O(nÂ²) allâ€‘pairs interactions.
- **Barnesâ€“Hut**: O(n log n) via a **quadtree** (2D) that approximates distant groups by their center of mass using a **Multipole Acceptance Criterion (MAC)**.
- Integration uses **Leapfrogâ€“Verlet** for stable time stepping.

---

## âš™ï¸ Methods
- **Sequential**
  - Build quadtree â†’ traverse with MAC to compute forces â†’ update positions/velocities (Leapfrogâ€“Verlet).
- **MPI Parallelization**
  - **Particle decomposition**: each rank updates a subset of particles.
  - Each rank builds a local Barnesâ€“Hut tree from global particle data.
  - Exchange updated particles each step with **`MPI_Allgatherv`**.
- **Performance-Oriented Changes**
  - **Iterative (nonâ€‘recursive) traversal** to avoid stack overflows on large inputs.
  - **Inline centerâ€‘ofâ€‘mass updates** during insertion to skip a separate aggregation pass.

---

## ğŸ“Š Results (summary)
Tested particles: **10**, **100**, **100,000**; processes: **1â€“20**.

- **Small (10)**: Parallel slower (MPI overhead dominates).
- **Medium (100)**: Best â‰ˆ **1.78Ã—** speedup at **4 processes**; >4 degrades (comm overhead).
- **Large (100,000)**: Best â‰ˆ **2.29Ã—** at **4 processes**; plateaus â‰ˆ **2.1â€“2.2Ã—** beyond that.
- **Bottlenecks**: global tree construction per rank (Amdahlâ€™s law), **`MPI_Allgatherv`** cost, and memory bandwidth contention.

---

## ğŸ–¥ï¸ Running
1) Build:
   ~~~bash
   make
   ~~~
2) Run (MPI):
   ~~~bash
   mpirun -np <num_procs> ./barnes_hut <options>
   ~~~
**Options**
- `-s <steps>`: number of iterations  
- `-t <theta>`: MAC threshold (smaller â†’ more accurate, slower)  
- `-d <dt>`: timestep  
- `-q`: run **sequential** implementation

---

## ğŸš€ Future Directions
- **Distributed tree construction** (each rank holds a partition; exchange summaries).
- **Spatial domain decomposition** to reduce communication.
- **Hybrid parallelism**: MPI + OpenMP/pthreads.

---

## ğŸ” Takeaways
- Practical intro to **MPI** collectives and data distribution.
- Clear view of **compute vs. communication** tradeâ€‘offs.
- Parallel speedup can be capped by **global dependencies** and **collective costs**.
